{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition Model using Tensorflow Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and show files in the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at the data file\n",
    "\n",
    "The data has been acquired from a kaggle and ICML competition : https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fer2013.csv\"\n",
    "label_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "df = pd.read_csv(filename, header=0, na_filter = False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting pixel data using the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name) :\n",
    "    # images are 48x48\n",
    "    # N = 35887\n",
    "    Y = []\n",
    "    X = []\n",
    "    first = True  \n",
    "    for line in open(file_name) :\n",
    "        if first :\n",
    "            first = False  # Not expracting the first line since it contains header\n",
    "        else :\n",
    "            row = line.split(',')\n",
    "            Y.append(int(row[0]))\n",
    "            X.append([int(p) for p in row[1].split()])\n",
    "    \n",
    "    X,Y = np.array(X) / 255.0, np.array(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = get_data(filename)\n",
    "num_class = len(set(Y))\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = X.shape\n",
    "X = X.reshape(N, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)\n",
    "y_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)   # To convert the labels \n",
    "y_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)     # to one-hot encoded lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN and run the model on train and test set\n",
    "\n",
    "![CNN Architecture implemented](model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (48,48,1)\n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolution Layer 1\n",
    "# ---- Convolution ---- Filters = 64 ---- Kernel_size = (3,3) ---- Padding = 'SAME'\n",
    "# ---- Batch normalization\n",
    "# ---- ReLU Activation\n",
    "# ---- Max Pooling 2D ---- Pool_size = (2,2)\n",
    "# ---- Dropout ---- Rate = 0.25\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', input_shape = (48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "# Convolution Layer 2\n",
    "# ---- Convolution ---- Filters = 128 ---- Kernel_size = (5,5) ---- Padding = 'SAME'\n",
    "# ---- Batch normalization\n",
    "# ---- ReLU Activation\n",
    "# ---- Max Pooling 2D ---- Pool_size = (2,2)\n",
    "# ---- Dropout ---- Rate = 0.25\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (5,5), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "# Convolution Layer 3\n",
    "# ---- Convolution ---- Filters = 512 ---- Kernel_size = (3,3) ---- Padding = 'SAME'\n",
    "# ---- Batch normalization\n",
    "# ---- ReLU Activation\n",
    "# ---- Max Pooling 2D ---- Pool_size = (2,2)\n",
    "# ---- Dropout ---- Rate = 0.25\n",
    "\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "# Convolution Layer 4\n",
    "# ---- Convolution ---- Filters = 512 ---- Kernel_size = (3,3) ---- Padding = 'SAME'\n",
    "# ---- Batch normalization\n",
    "# ---- ReLU Activation\n",
    "# ---- Max Pooling 2D ---- Pool_size = (2,2)\n",
    "# ---- Dropout ---- Rate = 0.25\n",
    "\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "# Flatten the activations for Fully connected layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected layer 2 ---- Dense : Units = 256 ---> Batch normalization ---> ReLU Activation ---> Dropout : Rate = 0.25\n",
    "\n",
    "model.add(Dense(units = 256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "# Fully Connected layer 2 ---- Dense : Units = 512 ---> Batch normalization ---> ReLU Activation ---> Dropout : Rate = 0.25\n",
    "\n",
    "model.add(Dense(units = 512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "\n",
    "# Softmax output layer\n",
    "model.add(Dense(units = 7, activation = 'softmax'))\n",
    "\n",
    "# Define the optimizer and compile\n",
    "opt = Adam(lr = 0.0005) # Tuned for faster runtime of model, can be change to achieve higher accuracy\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15 # Tuned for faster run-time, increasing will make the program run longer and giving better results\n",
    "steps_per_epoch = X_train.shape[0] // batch_size   # Taking floor division\n",
    "validation_steps = X_test.shape[0] // batch_size   # for calculating no. of mini-batches\n",
    "\n",
    "# We save the best weights into a .h5 file for further use in our browser app\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor = 'val_accuracy',  \n",
    "                            save_weights_only = True, mode = 'max', verbose = 1)\n",
    "\n",
    "# This helps to reduce our Learning rate in case our loss function hits a plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 2, min_lr = 0.0001, mode = 'auto')\n",
    "\n",
    "# PlotLosserKeras helps us plot Accuracy and loss line charts as the model progresses from epoch to epoch for debug purposes\n",
    "callbacks = [PlotLossesKeras(), checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = epochs,\n",
    "    validation_data = (X_test, y_test),\n",
    "    validation_steps = validation_steps,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model as json string\n",
    "(for further use in our browser app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
